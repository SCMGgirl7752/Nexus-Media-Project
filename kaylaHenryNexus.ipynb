{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nexus Media | Your World, United\n",
    "\n",
    "<img src=\"Nexus Project Media/Nexus Media Dark.png\" style=\"width:450px; height:auto; display:block; margin:auto;\" />\n",
    "\n",
    "#### Welcome to Nexus Media, the newest social media platform designed to bring everyone together. Connect with family, friends, patrons, and coworkersâ€”all in one place. Share your stories through blogs and vlogs, and discover new connections on your own terms. Join Nexus Media today and experience the future of social networking! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the process for visually mapping and creating a pipeline for the data from the Nexus Media platform. The information ingested will better help understand how customers are using the platform and help make decisions about what can be done to bring in more customers. The data compares four tables of information for this: Users, Posts, Engagement, and Conversions. Each of these help determine how to create a better product for our customers.\n",
    "\n",
    " We create the datasets to use with a python file, and have the data ready to ingest. Then we create functions within AWS to create s3 buckets, glue database, glue crawler, and connect everything with lake formation. We also run these functions to create the proper ETL pipeline. After confirming the processes, we will use redshift to extract the data and format it; making it ready for further analyzing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Process Flow\n",
    "\n",
    " Below is the Business Process Flow\n",
    "\n",
    " <img src=\"Nexus Project Media/Nexus BPF.png\" style=\"width:600px; height:auto; display:block; margin:auto;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Relational Diagram\n",
    "\n",
    "Below is the ERD for the Social Media Platform\n",
    "\n",
    "<img src=\"Nexus Project Media/Nexus ERD.png\" style=\"width:600px; height:auto; display:block; margin:auto;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nexus Media Database DDL"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CREATE TABLE Users (\n",
    "    UserID VARCHAR(36) PRIMARY KEY,\n",
    "    UserType VARCHAR(20),\n",
    "    Age INT,\n",
    "    Gender VARCHAR(20),\n",
    "    Location VARCHAR(100),\n",
    "    AccountCreationDate DATE,\n",
    "    LastLoginDate DATE,\n",
    "    TotalLogins INT,\n",
    "    PreferredDevice VARCHAR(20),\n",
    "    ActiveStatus VARCHAR(20)\n",
    ");\n",
    "\n",
    "CREATE TABLE Posts (\n",
    "    PostID VARCHAR(36) PRIMARY KEY,\n",
    "    UserID VARCHAR(36),\n",
    "    PostType VARCHAR(50),\n",
    "    TotalPosts INT,\n",
    "    LikesReceived INT,\n",
    "    CommentsReceived INT,\n",
    "    SharesReceived INT,\n",
    "    FOREIGN KEY (UserID) REFERENCES Users(UserID)\n",
    ");\n",
    "\n",
    "CREATE TABLE Engagements (\n",
    "    EngagementID VARCHAR(36) PRIMARY KEY,\n",
    "    UserID VARCHAR(36),\n",
    "    FriendsConnectionsCount INT,\n",
    "    GroupsJoined INT,\n",
    "    EventsAttended INT,\n",
    "    EngagementRate DECIMAL(3, 2),\n",
    "    FOREIGN KEY (UserID) REFERENCES Users(UserID)\n",
    ");\n",
    "\n",
    "CREATE TABLE Conversions (\n",
    "    ConversionID VARCHAR(36) PRIMARY KEY,\n",
    "    UserID VARCHAR(36),\n",
    "    ConversionStatus VARCHAR(20),\n",
    "    FOREIGN KEY (UserID) REFERENCES Users(UserID)\n",
    ");\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connections to Source Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are going to make the datasets for each table using the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### users.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets created successfully and saved as CSV files.\n",
      "                                 UserID  UserType  Age  Gender  \\\n",
      "0  7fddf168-ca79-4f72-b421-5dbb5abfcce4  Business   52  Female   \n",
      "1  1a4ba482-9001-4ed0-8a5e-9f78d228af4f  Personal   30  Female   \n",
      "2  d54846c8-4e0b-4e85-9eb1-7bf1fed22c42  Business   51  Female   \n",
      "3  c617c22a-337c-4fca-b661-964bf1d58d5b  Business   29  Female   \n",
      "4  6aff9a88-cb4f-49d0-8065-c1506c11376f  Business   55  Female   \n",
      "\n",
      "            Location AccountCreationDate LastLoginDate  TotalLogins  \\\n",
      "0      South Adamton          2023-05-24    2024-05-02           47   \n",
      "1         Chapmanton          2024-07-10    2024-12-04          253   \n",
      "2  West Michaelshire          2025-01-11    2024-07-28          370   \n",
      "3           Lake Pam          2025-02-19    2024-05-29          444   \n",
      "4        Williamston          2024-11-13    2024-11-25          388   \n",
      "\n",
      "  PreferredDevice ActiveStatus  \n",
      "0          Mobile       Active  \n",
      "1         Desktop     Inactive  \n",
      "2          Mobile       Active  \n",
      "3          Mobile     Inactive  \n",
      "4          Tablet     Inactive  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "\n",
    "# Initialize Faker library\n",
    "fake = Faker()\n",
    "\n",
    "\n",
    "# Number of records to generate\n",
    "num_records = 1000\n",
    "\n",
    "\n",
    "# Define user types\n",
    "user_types = ['Personal', 'Business']\n",
    "# Generate Users dataset\n",
    "users_data = {\n",
    "    'UserID': [fake.uuid4() for _ in range(num_records)],\n",
    "    'UserType': [random.choice(user_types) for _ in range(num_records)],\n",
    "    'Age': [random.randint(18, 65) for _ in range(num_records)],\n",
    "    'Gender': [random.choice(['Male', 'Female', 'Other']) for _ in range(num_records)],\n",
    "    'Location': [fake.city() for _ in range(num_records)],\n",
    "    'AccountCreationDate': [fake.date_between(start_date='-2y', end_date='today') for _ in range(num_records)],\n",
    "    'LastLoginDate': [fake.date_between(start_date='-1y', end_date='today') for _ in range(num_records)],\n",
    "    'TotalLogins': [random.randint(1, 500) for _ in range(num_records)],\n",
    "    'PreferredDevice': [random.choice(['Mobile', 'Desktop', 'Tablet']) for _ in range(num_records)],\n",
    "    'ActiveStatus': [random.choice(['Active', 'Inactive']) for _ in range(num_records)]\n",
    "}\n",
    "\n",
    "# Create DataFrames\n",
    "users_df = pd.DataFrame(users_data)\n",
    "\n",
    "# Save to CSV\n",
    "users_df.to_csv(f'C:/users_dataset.csv', index=False)\n",
    "\n",
    "print(\"Datasets created successfully and saved as CSV files.\")\n",
    "print(users_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### posts.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets created successfully and saved as CSV files.\n",
      "                                 PostID                                UserID  \\\n",
      "0  e66008e5-2b02-4198-949c-842a024e3bd7  94f39491-0097-4aef-8066-9b1cca7d0b30   \n",
      "1  d7176723-ba6b-4f5e-9b27-4f56444144b2  bd28ae47-96c8-437b-9908-93f5752579a9   \n",
      "2  16dfef67-b5b2-4aca-8e73-efd829059d42  6e065bb9-e857-479c-bd1d-face81a93730   \n",
      "3  79ce3924-1703-49cb-95f5-416e7542b3d0  bd28ae47-96c8-437b-9908-93f5752579a9   \n",
      "4  4c1a44aa-58eb-4a9e-9a83-a262b1f9378d  43451613-f318-49d9-a06b-802e28052619   \n",
      "\n",
      "          PostType  TotalPosts  LikesReceived  CommentsReceived  \\\n",
      "0  Business Update         253            152              1358   \n",
      "1  Business Update         588           3654               946   \n",
      "2  Business Update          74           2225              1852   \n",
      "3  Personal Update         935            748               751   \n",
      "4  Business Update         279           3338               103   \n",
      "\n",
      "   SharesReceived  \n",
      "0             418  \n",
      "1             467  \n",
      "2             719  \n",
      "3             436  \n",
      "4             534  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# Initialize Faker library\n",
    "fake = Faker()\n",
    "\n",
    "# Number of records to generate\n",
    "num_records = 1000\n",
    "\n",
    "# Define post types\n",
    "post_types = ['Personal Update', 'Business Update', 'Promotional Content']\n",
    "\n",
    "# Generate Posts dataset\n",
    "posts_data = {\n",
    "    'PostID': [fake.uuid4() for _ in range(num_records)],\n",
    "    'UserID': [random.choice(users_data['UserID']) for _ in range(num_records)],\n",
    "    'PostType': [random.choice(post_types) for _ in range(num_records)],\n",
    "    'TotalPosts': [random.randint(1, 1000) for _ in range(num_records)],\n",
    "    'LikesReceived': [random.randint(0, 5000) for _ in range(num_records)],\n",
    "    'CommentsReceived': [random.randint(0, 2000) for _ in range(num_records)],\n",
    "    'SharesReceived': [random.randint(0, 1000) for _ in range(num_records)]\n",
    "}\n",
    "\n",
    "# Create DataFrames\n",
    "posts_df = pd.DataFrame(posts_data)\n",
    "\n",
    "# Save to CSV\n",
    "posts_df.to_csv(f'C:/posts_dataset.csv', index=False)\n",
    "\n",
    "print(\"Datasets created successfully and saved as CSV files.\")\n",
    "print(posts_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### engagement.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets created successfully and saved as CSV files.\n",
      "                           EngagementID                                UserID  \\\n",
      "0  8951a921-5178-4010-ba5e-46298e95acdd  74c5cfb7-21aa-47e9-8a7f-588af8f6b50e   \n",
      "1  c9ee0c83-2223-4f63-a74e-291906ab10fc  01c2e0a5-08a5-48a2-8c26-b6b9ff6cd01e   \n",
      "2  ddd4d3e4-5de8-427d-b606-67c9e2ca1905  1cada54c-ffe2-463e-9227-0cec1593d5ce   \n",
      "3  6abe2852-270e-4ca8-802d-de37f68d5076  bcd849fb-c2ee-40f0-b414-17bd01abd708   \n",
      "4  a260ed7e-758b-497a-9dc2-c00b29e95e0e  bf62bd1b-0781-4cfb-957a-bbd5571d1e1e   \n",
      "\n",
      "   FriendsConnectionsCount  GroupsJoined  EventsAttended  EngagementRate  \n",
      "0                     1928            24              91            0.10  \n",
      "1                      861            37              39            0.99  \n",
      "2                      604             4              58            0.40  \n",
      "3                     1589            50              21            0.29  \n",
      "4                     1222            39              87            0.13  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# Initialize Faker library\n",
    "fake = Faker()\n",
    "\n",
    "# Number of records to generate\n",
    "num_records = 1000\n",
    "\n",
    "# Generate Engagements dataset\n",
    "engagements_data = {\n",
    "    'EngagementID': [fake.uuid4() for _ in range(num_records)],\n",
    "    'UserID': [random.choice(users_data['UserID']) for _ in range(num_records)],\n",
    "    'FriendsConnectionsCount': [random.randint(1, 2000) for _ in range(num_records)],\n",
    "    'GroupsJoined': [random.randint(0, 50) for _ in range(num_records)],\n",
    "    'EventsAttended': [random.randint(0, 100) for _ in range(num_records)],\n",
    "    'EngagementRate': [round(random.uniform(0, 1), 2) for _ in range(num_records)]\n",
    "}\n",
    "\n",
    "# Create DataFrames\n",
    "engagements_df = pd.DataFrame(engagements_data)\n",
    "\n",
    "# Save to CSV\n",
    "engagements_df.to_csv(f'C:/engagements_dataset.csv', index=False)\n",
    "\n",
    "print(\"Datasets created successfully and saved as CSV files.\")\n",
    "print(engagements_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### conversions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets created successfully and saved as CSV files.\n",
      "                           ConversionID                                UserID  \\\n",
      "0  83f3987e-3940-4f02-8529-ca2bebc5bb0e  5344521e-2e71-4512-8116-767b1a968779   \n",
      "1  029c1568-a658-4d37-9cfe-47e0f2210d56  bcdf3a1f-ec5d-4451-9ef9-b104a0c349e3   \n",
      "2  74441cdc-f605-491e-b615-834d8a3b1bd4  c8b497e1-3db8-4394-87fc-ba40987d284d   \n",
      "3  07c7e982-998f-47a0-91b9-126427e0701d  0707a505-080e-46fd-9ae3-eea04b4479e8   \n",
      "4  89bde51f-903a-4e4a-80b5-d008c0e74e37  d764e801-3a5d-41df-abb4-a682b75527e4   \n",
      "\n",
      "  ConversionStatus  \n",
      "0        Converted  \n",
      "1    Not Converted  \n",
      "2    Not Converted  \n",
      "3        Converted  \n",
      "4        Converted  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# Initialize Faker library\n",
    "fake = Faker()\n",
    "\n",
    "# Number of records to generate\n",
    "num_records = 1000\n",
    "\n",
    "# Generate Conversions dataset\n",
    "conversions_data = {\n",
    "    'ConversionID': [fake.uuid4() for _ in range(num_records)],\n",
    "    'UserID': [random.choice(users_data['UserID']) for _ in range(num_records)],\n",
    "    'ConversionStatus': [random.choice(['Converted', 'Not Converted']) for _ in range(num_records)]\n",
    "}\n",
    "\n",
    "# Create DataFrames\n",
    "conversions_df = pd.DataFrame(conversions_data)\n",
    "\n",
    "# Save to CSV\n",
    "conversions_df.to_csv(f'C:/conversions_dataset.csv', index=False)\n",
    "\n",
    "print(\"Datasets created successfully and saved as CSV files.\")\n",
    "print(conversions_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create S3 bucket for all Raw and Verified Data.\n",
    "\n",
    "\n",
    "\n",
    "First we will create a fuction to simplify the process of creating a bucket. Then we will run the fuction to create the seperate buckets that we are going to want.\n",
    "\n",
    "Make another function to upload the chosen files to a specified bucket, then run the function to ingest the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create S3 Bucket function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "import boto3\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "\n",
    "def create_bucket(bucket_name, region=None):\n",
    "    try:\n",
    "        s3 = boto3.client('s3')\n",
    "        s3.create_bucket(\n",
    "            Bucket=bucket_name,\n",
    "            CreateBucketConfiguration={\n",
    "                'LocationConstraint': region\n",
    "            }\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        print(f\"Error: {e}\")\n",
    "        return False\n",
    "    return f\"Bucket {bucket_name} created in {region} region!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Bucket function for raw and verified data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket kh-nexus-raw created in us-west-2 region!\n",
      "Bucket kh-nexus-verified created in us-west-2 region!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(create_bucket(\"kh-nexus-raw\", \"us-west-2\"))\n",
    "\n",
    "print(create_bucket(\"kh-nexus-verified\", \"us-west-2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create function to upload files to S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def upload_file(file_name, bucket, object_name=None):\n",
    "    if object_name is None:\n",
    "        object_name = file_name\n",
    "    s3 = boto3.client('s3')\n",
    "    try:\n",
    "        response = s3.upload_file(file_name, bucket, object_name)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        print(f\"Error: {e}\")\n",
    "        return False\n",
    "    return f\"File uploaded successfully to {bucket} bucket!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run upload function for csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'File uploaded successfully to kh-nexus-raw bucket!'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#call the function to upload CSV file\n",
    "upload_file(\"users_dataset.csv\", \"kh-nexus-raw\", \"users_dataset.csv\")\n",
    "upload_file(\"posts_dataset.csv\", \"kh-nexus-raw\", \"posts_dataset.csv\")\n",
    "upload_file(\"engagements_dataset.csv\", \"kh-nexus-raw\", \"engagements_dataset.csv\")\n",
    "upload_file(\"conversions_dataset.csv\", \"kh-nexus-raw\", \"conversions_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Glue Database and Lake Formation\n",
    "\n",
    "\n",
    "Connect the lake formation to the S3 bucket and Glue database, to prepare for verifying the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create function to make Glue Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_glue_database(database_name):\n",
    "    glue = boto3.client('glue')\n",
    "    try:\n",
    "        glue.create_database(\n",
    "            DatabaseInput={\n",
    "                'Name': database_name\n",
    "            }\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        print(f\"Error: {e}\")\n",
    "        return False\n",
    "    return f\"Database {database_name} created successfully!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Glue Database function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database NexusDatabase-kh created successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(create_glue_database(\"NexusDatabase-kh\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lake Formation Integration for S3 bucket and Glue database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:An error occurred (AlreadyExistsException) when calling the RegisterResource operation: Resource is already registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: An error occurred (AlreadyExistsException) when calling the RegisterResource operation: Resource is already registered\n",
      "False\n",
      "Resource arn:aws:s3:::kh-nexus-verified registered successfully with Lake Formation!\n",
      "Permissions granted to arn:aws:iam:::role/service-role/AWSGlueServiceRole-s for database NexusDatabase-kh!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def register_resource_with_lake_formation(bucket_name, iam_role_arn):\n",
    "    try:\n",
    "        lakeformation = boto3.client('lakeformation')\n",
    "        resource_arn = f'arn:aws:s3:::{bucket_name}'\n",
    "\n",
    "        # Register the S3 path with Lake Formation\n",
    "        lakeformation.register_resource(\n",
    "            ResourceArn=resource_arn,\n",
    "            RoleArn=iam_role_arn\n",
    "        )\n",
    "        return f\"Resource {resource_arn} registered successfully with Lake Formation!\"\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        print(f\"Error: {e}\")\n",
    "        return False\n",
    "\n",
    "def grant_permissions_lake_formation(database_name, principal_arn):\n",
    "    try:\n",
    "        lakeformation = boto3.client('lakeformation')\n",
    "\n",
    "        # Grant Lake Formation permissions\n",
    "        lakeformation.grant_permissions(\n",
    "            Principal={'DataLakePrincipalIdentifier': principal_arn},\n",
    "            Resource={\n",
    "                'Database': {\n",
    "                    'Name': database_name\n",
    "                }\n",
    "            },\n",
    "            Permissions=['ALL'],['DATA_LOCATION_ACCESS']\n",
    "        )\n",
    "        return f\"Permissions granted to {principal_arn} for database {database_name}!\"\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        print(f\"Error: {e}\")\n",
    "        return False\n",
    "\n",
    "iam_role_arn = \"arn:aws:iam:::role/aws-service-role/lakeformation.amazonaws.com/AWSServiceRoleForLakeFormationDataAccess\"\n",
    "principal_arn = \"arn:aws:iam:::role/service-role/AWSGlueServiceRole-s\"\n",
    "\n",
    "# Register the S3 resources with Lake Formation\n",
    "print(register_resource_with_lake_formation(\"kh-nexus-raw\", iam_role_arn))\n",
    "print(register_resource_with_lake_formation(\"kh-nexus-verified\", iam_role_arn))\n",
    "\n",
    "# Grant permissions in Lake Formation\n",
    "print(grant_permissions_lake_formation(\"NexusDatabase-kh\", principal_arn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Glue Crawler \n",
    "\n",
    "Create a Glue Crawler to Extract the data from the S3 bucket of raw data into the Glue Catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function to run the glue crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def create_glue_crawler(crawler_name, role, database_name, s3_paths):\n",
    "    client = boto3.client('glue')\n",
    "    \n",
    "    try:\n",
    "        response = client.create_crawler(\n",
    "            Name=crawler_name,\n",
    "            Role=role,\n",
    "            DatabaseName=database_name,\n",
    "            Targets={\n",
    "                'S3Targets': [{'Path': path} for path in s3_paths]\n",
    "            },\n",
    "            TablePrefix='',\n",
    "            SchemaChangePolicy={\n",
    "                'UpdateBehavior': 'UPDATE_IN_DATABASE',\n",
    "                'DeleteBehavior': 'LOG'\n",
    "            }\n",
    "        )\n",
    "        return f\"Crawler {crawler_name} created successfully!\"\n",
    "    except ClientError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return False\n",
    "\n",
    "def start_glue_crawler(crawler_name):\n",
    "    client = boto3.client('glue')\n",
    "    \n",
    "    try:\n",
    "        response = client.start_crawler(Name=crawler_name)\n",
    "        return f\"Crawler {crawler_name} started successfully!\"\n",
    "    except ClientError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Glue Grawler function for extracting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: An error occurred (AlreadyExistsException) when calling the CreateCrawler operation::nexus_crawler already exists\n",
      "False\n",
      "Crawler nexus_crawler started successfully!\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "crawler_name = 'nexus_crawler'\n",
    "role = 'arn:aws:iam:::role/service-role/AWSGlueServiceRole-s'\n",
    "database_name = 'NexusDatabase-kh'\n",
    "s3_paths = [\n",
    "    's3://kh-nexus-raw/users_dataset/',\n",
    "    's3://kh-nexus-raw/engagements_dataset/',\n",
    "    's3://kh-nexus-raw/posts_dataset/',\n",
    "    's3://kh-nexus-raw/conversions_dataset/'\n",
    "]\n",
    "\n",
    "# Create and start the crawler\n",
    "print(create_glue_crawler(crawler_name, role, database_name, s3_paths))\n",
    "print(start_glue_crawler(crawler_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform and verify data\n",
    "\n",
    "Glue ETL jobs to correct mapping mistakes and covnvert csv to parquet format. Following jobs were ran within the AWS Glue concole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users verified"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsgluedq.transforms import EvaluateDataQuality\n",
    "\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# Default ruleset used by all target nodes with data quality enabled\n",
    "DEFAULT_DATA_QUALITY_RULESET = \"\"\"\n",
    "    Rules = [\n",
    "        ColumnCount > 0\n",
    "    ]\n",
    "\"\"\"\n",
    "\n",
    "# Script generated for node AWS Glue Data Catalog\n",
    "AWSGlueDataCatalog_node1741284064354 = glueContext.create_dynamic_frame.from_catalog(\n",
    "    database=\"nexusdatabase-kh\", \n",
    "    table_name=\"users_dataset\", \n",
    "    transformation_ctx=\"AWSGlueDataCatalog_node1741284064354\"\n",
    "    )\n",
    "\n",
    "# Script generated for node Change Schema\n",
    "ChangeSchema_node1741284109180 = ApplyMapping.apply(\n",
    "    frame=AWSGlueDataCatalog_node1741284064354,\n",
    "     mappings=[\n",
    "        (\"userid\", \"string\", \"userid\", \"string\"), \n",
    "        (\"usertype\", \"string\", \"usertype\", \"string\"), \n",
    "        (\"age\", \"long\", \"age\", \"string\"), \n",
    "        (\"gender\", \"string\", \"gender\", \"string\"), \n",
    "        (\"location\", \"string\", \"location\", \"string\"), \n",
    "        (\"accountcreationdate\", \"string\", \"accountcreationdate\", \"string\"), \n",
    "        (\"lastlogindate\", \"string\", \"lastlogindate\", \"string\"), \n",
    "        (\"totallogins\", \"long\", \"totallogins\", \"string\"), \n",
    "        (\"preferreddevice\", \"string\", \"preferreddevice\", \"string\"), \n",
    "        (\"activestatus\", \"string\", \"activestatus\", \"string\")\n",
    "        ], \n",
    "    transformation_ctx=\"ChangeSchema_node1741284109180\")\n",
    "\n",
    "# Script generated for node Amazon S3\n",
    "EvaluateDataQuality().process_rows(\n",
    "    frame=ChangeSchema_node1741284109180, \n",
    "    ruleset=DEFAULT_DATA_QUALITY_RULESET, \n",
    "    publishing_options={\n",
    "        \"dataQualityEvaluationContext\": \n",
    "            \"EvaluateDataQuality_node1741283963615\", \n",
    "            \"enableDataQualityResultsPublishing\": \n",
    "                True}, \n",
    "    additional_options={\n",
    "        \"dataQualityResultsPublishing.strategy\": \n",
    "            \"BEST_EFFORT\", \n",
    "            \"observations.scope\": \n",
    "                \"ALL\"\n",
    "        })\n",
    "AmazonS3_node1741284223272 = glueContext.write_dynamic_frame.from_options(\n",
    "    frame=ChangeSchema_node1741284109180, \n",
    "    connection_type=\"s3\", \n",
    "    format=\"glueparquet\", \n",
    "    connection_options={\n",
    "        \"path\": \n",
    "            \"s3://kh-nexus-verified/Users/\", \n",
    "            \"partitionKeys\": [\n",
    "        ]},\n",
    "     format_options={\n",
    "        \"compression\": \n",
    "            \"snappy\"\n",
    "        }, \n",
    "    transformation_ctx=\"AmazonS3_node1741284223272\")\n",
    "\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posts verified"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsgluedq.transforms import EvaluateDataQuality\n",
    "\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# Default ruleset used by all target nodes with data quality enabled\n",
    "DEFAULT_DATA_QUALITY_RULESET = \"\"\"\n",
    "    Rules = [\n",
    "        ColumnCount > 0\n",
    "    ]\n",
    "\"\"\"\n",
    "\n",
    "# Script generated for node AWS Glue Data Catalog\n",
    "AWSGlueDataCatalog_node1741285752761 = glueContext.create_dynamic_frame.from_catalog(database=\"nexusdatabase-kh\", table_name=\"posts_dataset\", transformation_ctx=\"AWSGlueDataCatalog_node1741285752761\")\n",
    "\n",
    "# Script generated for node Change Schema\n",
    "ChangeSchema_node1741285755366 = ApplyMapping.apply(frame=AWSGlueDataCatalog_node1741285752761, mappings=[(\"postid\", \"string\", \"postid\", \"string\"), (\"userid\", \"string\", \"userid\", \"string\"), (\"posttype\", \"string\", \"posttype\", \"string\"), (\"totalposts\", \"long\", \"totalposts\", \"string\"), (\"likesreceived\", \"long\", \"likesreceived\", \"string\"), (\"commentsreceived\", \"long\", \"commentsreceived\", \"string\"), (\"sharesreceived\", \"long\", \"sharesreceived\", \"string\")], transformation_ctx=\"ChangeSchema_node1741285755366\")\n",
    "\n",
    "# Script generated for node Amazon S3\n",
    "EvaluateDataQuality().process_rows(frame=ChangeSchema_node1741285755366, ruleset=DEFAULT_DATA_QUALITY_RULESET, publishing_options={\"dataQualityEvaluationContext\": \"EvaluateDataQuality_node1741285649378\", \"enableDataQualityResultsPublishing\": True}, additional_options={\"dataQualityResultsPublishing.strategy\": \"BEST_EFFORT\", \"observations.scope\": \"ALL\"})\n",
    "AmazonS3_node1741285757918 = glueContext.write_dynamic_frame.from_options(frame=ChangeSchema_node1741285755366, connection_type=\"s3\", format=\"glueparquet\", connection_options={\"path\": \"s3://kh-nexus-verified/Posts/\", \"partitionKeys\": []}, format_options={\"compression\": \"snappy\"}, transformation_ctx=\"AmazonS3_node1741285757918\")\n",
    "\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engagement verified "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsgluedq.transforms import EvaluateDataQuality\n",
    "\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# Default ruleset used by all target nodes with data quality enabled\n",
    "DEFAULT_DATA_QUALITY_RULESET = \"\"\"\n",
    "    Rules = [\n",
    "        ColumnCount > 0\n",
    "    ]\n",
    "\"\"\"\n",
    "\n",
    "# Script generated for node AWS Glue Data Catalog\n",
    "AWSGlueDataCatalog_node1741286075686 = glueContext.create_dynamic_frame.from_catalog(database=\"nexusdatabase-kh\", table_name=\"engagements_dataset\", transformation_ctx=\"AWSGlueDataCatalog_node1741286075686\")\n",
    "\n",
    "# Script generated for node Change Schema\n",
    "ChangeSchema_node1741286112823 = ApplyMapping.apply(frame=AWSGlueDataCatalog_node1741286075686, mappings=[(\"engagementid\", \"string\", \"engagementid\", \"string\"), (\"userid\", \"string\", \"userid\", \"string\"), (\"friendsconnectionscount\", \"long\", \"friendsconnectionscount\", \"string\"), (\"groupsjoined\", \"long\", \"groupsjoined\", \"string\"), (\"eventsattended\", \"long\", \"eventsattended\", \"string\"), (\"engagementrate\", \"double\", \"engagementrate\", \"string\")], transformation_ctx=\"ChangeSchema_node1741286112823\")\n",
    "\n",
    "# Script generated for node Amazon S3\n",
    "EvaluateDataQuality().process_rows(frame=ChangeSchema_node1741286112823, ruleset=DEFAULT_DATA_QUALITY_RULESET, publishing_options={\"dataQualityEvaluationContext\": \"EvaluateDataQuality_node1741285649378\", \"enableDataQualityResultsPublishing\": True}, additional_options={\"dataQualityResultsPublishing.strategy\": \"BEST_EFFORT\", \"observations.scope\": \"ALL\"})\n",
    "AmazonS3_node1741286136325 = glueContext.write_dynamic_frame.from_options(frame=ChangeSchema_node1741286112823, connection_type=\"s3\", format=\"glueparquet\", connection_options={\"path\": \"s3://kh-nexus-verified/Engagements/\", \"partitionKeys\": []}, format_options={\"compression\": \"snappy\"}, transformation_ctx=\"AmazonS3_node1741286136325\")\n",
    "\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversions verified "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsgluedq.transforms import EvaluateDataQuality\n",
    "\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# Default ruleset used by all target nodes with data quality enabled\n",
    "DEFAULT_DATA_QUALITY_RULESET = \"\"\"\n",
    "    Rules = [\n",
    "        ColumnCount > 0\n",
    "    ]\n",
    "\"\"\"\n",
    "\n",
    "# Script generated for node AWS Glue Data Catalog\n",
    "AWSGlueDataCatalog_node1741286321310 = glueContext.create_dynamic_frame.from_catalog(database=\"nexusdatabase-kh\", table_name=\"conversions_dataset\", transformation_ctx=\"AWSGlueDataCatalog_node1741286321310\")\n",
    "\n",
    "# Script generated for node Change Schema\n",
    "ChangeSchema_node1741286338937 = ApplyMapping.apply(frame=AWSGlueDataCatalog_node1741286321310, mappings=[(\"col0\", \"string\", \"conversionid\", \"string\"), (\"col1\", \"string\", \"userid\", \"string\"), (\"col2\", \"string\", \"conversionsstatus\", \"string\")], transformation_ctx=\"ChangeSchema_node1741286338937\")\n",
    "\n",
    "# Script generated for node Amazon S3\n",
    "EvaluateDataQuality().process_rows(frame=ChangeSchema_node1741286338937, ruleset=DEFAULT_DATA_QUALITY_RULESET, publishing_options={\"dataQualityEvaluationContext\": \"EvaluateDataQuality_node1741285649378\", \"enableDataQualityResultsPublishing\": True}, additional_options={\"dataQualityResultsPublishing.strategy\": \"BEST_EFFORT\", \"observations.scope\": \"ALL\"})\n",
    "AmazonS3_node1741286421498 = glueContext.write_dynamic_frame.from_options(frame=ChangeSchema_node1741286338937, connection_type=\"s3\", format=\"glueparquet\", connection_options={\"path\": \"s3://kh-nexus-verified/Conversions/\", \"partitionKeys\": []}, format_options={\"compression\": \"snappy\"}, transformation_ctx=\"AmazonS3_node1741286421498\")\n",
    "\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Verified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redshift Warehouse DDL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The last thing we are going to do is connect and create a database in Redshift, to prepare and allow for quering. The following processes were ran within the query editor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the Schema within the database"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "CREATE EXTERNAL SCHEMA NexusCapstone\n",
    "drop table dev.nexuscapstone.sales_data;\n",
    "FROM data catalog\n",
    "DATABASE 'dev'\n",
    "IAM_ROLE 'arn:aws:iam:::role/spectrumRole'\n",
    "CREATE EXTERNAL DATABASE IF NOT EXISTS;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create tables and load the data into the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Create Users table\n",
    "CREATE EXTERNAL TABLE nexusCapstone.users (\n",
    "    UserID VARCHAR(36),\n",
    "    UserType VARCHAR(20),\n",
    "    Age VARCHAR(5),\n",
    "    Gender VARCHAR(20),\n",
    "    Location VARCHAR(100),\n",
    "    AccountCreationDate VARCHAR(10),\n",
    "    LastLoginDate VARCHAR(10),\n",
    "    TotalLogins VARCHAR(20),\n",
    "    PreferredDevice VARCHAR(20),\n",
    "    ActiveStatus VARCHAR(20)\n",
    ")\n",
    "STORED AS PARQUET\n",
    "LOCATION 's3://kh-nexus-verified/Users/';\n",
    "\n",
    "\n",
    "-- Create Posts table\n",
    "CREATE EXTERNAL TABLE nexusCapstone.posts (\n",
    "    PostID VARCHAR(36),\n",
    "    UserID VARCHAR(36),\n",
    "    PostType VARCHAR(50),\n",
    "    TotalPosts VARCHAR(10),\n",
    "    LikesReceived VARCHAR(10),\n",
    "    CommentsReceived VARCHAR(50),\n",
    "    SharesReceived VARCHAR(10)\n",
    ")\n",
    "STORED AS PARQUET\n",
    "LOCATION 's3://kh-nexus-verified/Posts/';\n",
    "\n",
    "-- Create Engagements table\n",
    "CREATE EXTERNAL TABLE nexuscapstone.engagements (\n",
    "    EngagementID VARCHAR(36),\n",
    "    UserID VARCHAR(36),\n",
    "    FriendsConnectionsCount VARCHAR(10),\n",
    "    GroupsJoined VARCHAR(10),\n",
    "    EventsAttended VARCHAR(10),\n",
    "    EngagementRate VARCHAR(10)\n",
    ")\n",
    "STORED AS PARQUET\n",
    "LOCATION 's3://kh-nexus-verified/Engagements/';\n",
    "\n",
    "-- Create Conversions table\n",
    "CREATE EXTERNAL TABLE nexusCapstone.conversions (\n",
    "    ConversionID VARCHAR(36),\n",
    "    UserID VARCHAR(36),\n",
    "    ConversionStatus VARCHAR(20)\n",
    ")\n",
    "STORED AS PARQUET\n",
    "LOCATION 's3://kh-nexus-verified/Conversions/';\n",
    "\n",
    "-- Verify data\n",
    "SELECT * FROM nexusCapstone.users;\n",
    "SELECT * FROM nexusCapstone.posts;\n",
    "SELECT * FROM nexusCapstone.engagements;\n",
    "SELECT * FROM nexusCapstone.conversions;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Verify that data has transfered into the database correctly"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "SELECT COUNT(*) FROM nexusCapstone.users;\n",
    "SELECT COUNT(*) FROM nexusCapstone.posts;\n",
    "SELECT COUNT(*) FROM nexusCapstone.engagements;\n",
    "SELECT COUNT(*) FROM nexusCapstone.conversions;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data is ingested, transformed, and ready for future queries."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "-- Verify data\n",
    "SELECT * FROM nexusCapstone.users limit 10;\n",
    "SELECT * FROM nexusCapstone.posts limit 10;\n",
    "SELECT * FROM nexusCapstone.engagements limit 10;\n",
    "SELECT * FROM nexusCapstone.conversions limit 10;\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
